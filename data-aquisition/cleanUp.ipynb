{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "In this Notebook the Scraped data is cleaned up, this step is done after collecting the pictures.\n",
    "\n",
    "First the instances that did not include a valid thumbnail link are removed (missing.txt in each subset folder)\n",
    "\n",
    "Second the title text is cleaned and standardized to make it ready for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import time\n",
    "from urllib.error import HTTPError\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import emoji\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove instances with missing \n",
    "\n",
    "dataPath = './DataFiles/'\n",
    "imagePath = './Images/'\n",
    "cleanPath = './CleanedFiles/'\n",
    "\n",
    "fileNames = os.listdir(dataPath)\n",
    "\n",
    "for fileName in fileNames:\n",
    "    \n",
    "    dataFile = pd.read_csv(dataPath + fileName, lineterminator='\\n')\n",
    "    missingList = pd.read_csv(imagePath + fileName.replace('.csv','/') + 'missing.txt', lineterminator='\\n',header=None)[0]\n",
    "    noMissing = dataFile[~dataFile['ID'].isin(missingList)]\n",
    "    noMissing.to_csv(cleanPath + fileName, index=False, sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Private and Missing Videos\n",
    "\n",
    "fileNames = os.listdir(cleanPath)\n",
    "\n",
    "titleBlacklist = ['Private video', 'Deleted video']\n",
    "\n",
    "for fileName in fileNames:\n",
    "         \n",
    "    \n",
    "    dataFile = pd.read_csv(cleanPath + fileName, lineterminator='\\n')\n",
    "    noMissing = dataFile[~dataFile['TITLE'].isin(titleBlacklist)]\n",
    "    noMissing.to_csv(cleanPath + fileName, index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up strings to alphanumeric characters only\n",
    "\n",
    "fileNames = os.listdir(cleanPath)\n",
    "\n",
    "for fileName in fileNames:\n",
    "    # read one file\n",
    "    dataFile = pd.read_csv(cleanPath + fileName, lineterminator='\\n')\n",
    "    \n",
    "    # replace emojis with their text equivalent\n",
    "    \n",
    "    dataFile['TITLE'] = dataFile['TITLE'].apply(emoji.demojize, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    #all strings to lowercase\n",
    "    dataFile['TITLE'] = dataFile['TITLE'].str.lower()\n",
    "    \n",
    "    #remove everything except a-z 0-9 and ' '.\n",
    "    dataFile['TITLE'] = dataFile['TITLE'].replace('[^a-zA-Z0-9 ]', ' ', regex=True)\n",
    "    \n",
    "    #remove reduce consecutive spaces to a single space\n",
    "    dataFile['TITLE'] = dataFile['TITLE'].replace(' +', ' ', regex=True)\n",
    "    \n",
    "    #remove leading space\n",
    "    dataFile['TITLE'] = dataFile['TITLE'].replace('^ ', '', regex=True)\n",
    "    \n",
    "    #remove leading space\n",
    "    dataFile['TITLE'] = dataFile['TITLE'].replace(' $', '', regex=True)\n",
    "    \n",
    "    #remove all rows with no title\n",
    "    dataFile = dataFile[dataFile['TITLE'] != '']\n",
    "    \n",
    "    #save the data frame\n",
    "    dataFile.to_csv(cleanPath + fileName, index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "31\n",
      "30\n",
      "28\n",
      "33\n",
      "31\n",
      "35\n",
      "44\n",
      "45\n",
      "41\n",
      "36\n",
      "33\n",
      "28\n",
      "36\n",
      "36\n",
      "22\n",
      "45\n",
      "29\n",
      "37\n",
      "35\n",
      "31\n",
      "38\n",
      "63\n",
      "37\n",
      "36\n",
      "52\n",
      "34\n",
      "43\n",
      "29\n",
      "27\n",
      "41\n",
      "38\n",
      "38\n",
      "38\n",
      "37\n",
      "31\n",
      "28\n",
      "31\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# By checking the maximum number of white spaces in title in each \n",
    "# data set we can count how many words the the largest title includes\n",
    "\n",
    "fileNames = os.listdir(cleanPath)\n",
    "count = 1\n",
    "\n",
    "for fileName in fileNames:\n",
    "    dataFile = pd.read_csv(cleanPath + fileName, lineterminator='\\n')\n",
    "\n",
    "    print(dataFile['TITLE'].str.count(' ').max())\n",
    "    \n",
    "# the maximum number of whitespaces is 63 so the title with the most amount of words has 64 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
