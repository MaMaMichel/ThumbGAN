{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping using the YT API\n",
    "\n",
    "This notebook containes the different approaches to scrpaping video IDs, titles and thumbnail URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "#open key file and read in api key\n",
    "\n",
    "with open (\"key\", \"r\") as myfile:\n",
    "    api_key=myfile.read()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Videos by Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "page: 0\n",
      "page: 1\n",
      "page: 2\n",
      "page: 3\n",
      "page: 4\n",
      "page: 5\n",
      "page: 6\n",
      "page: 7\n",
      "page: 8\n",
      "page: 9\n",
      "page: 10\n",
      "page: 11\n",
      "page: 12\n",
      "page: 13\n",
      "page: 14\n",
      "done\n",
      "saving...\n",
      "saved...\n"
     ]
    }
   ],
   "source": [
    "# Scraping youtube video id and title using the YT Api search function. \n",
    "# Quota cost: 100 per search (each next page request counts as a search)\n",
    "\n",
    "\n",
    "#opening the google API service using the api-key\n",
    "\n",
    "searchData = []\n",
    "with build('youtube', 'v3', developerKey=api_key) as youtube:\n",
    "    request = youtube.search().list(part='id,snippet',\n",
    "                                    q='try not to laugh', \n",
    "                                    fields = 'nextPageToken,items(id,snippet)',\n",
    "                                    maxResults = 50\n",
    "                                    )\n",
    "    \n",
    "    i = 0\n",
    "    print('Start')\n",
    "    while request and i < 40:\n",
    "        response = request.execute()\n",
    "        print('page: ' + str(i))\n",
    "        \n",
    "\n",
    "        for result in response['items']:\n",
    "            if result['id']['kind'] == 'youtube#video':\n",
    "                vidId = result['id']['videoId']\n",
    "                searchData.append([vidId, result['snippet']['title'], \n",
    "                           'https://i.ytimg.com/vi/' + vidId + '/default.jpg', \n",
    "                           'https://i.ytimg.com/vi/' + vidId + '/mqdefault.jpg',\n",
    "                           'https://i.ytimg.com/vi/' + vidId + '/hqdefault.jpg']\n",
    "                         )\n",
    "\n",
    "        request = youtube.search().list_next(\n",
    "            request, response)\n",
    "        i += 1\n",
    "        \n",
    "    print('done')\n",
    "\n",
    "print('saving...')\n",
    "pd.DataFrame(searchData, columns = ['ID', 'TITLE', 'THUMBNAIL1','THUMBNAIL2','THUMBNAIL3']).to_csv('searchData' + str(round(time.time())) + '.csv', index=False)\n",
    "print('saved...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Popular',\n",
       " 'Songs',\n",
       " 'Mix',\n",
       " '2021',\n",
       " 'Camila Cabello',\n",
       " 'Ed Sheeran',\n",
       " 'Kygo',\n",
       " 'Style',\n",
       " 'Chill',\n",
       " 'Chill Out']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['items'][1]['snippet']['tags'][:10]\n",
    "#response['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 'youtube#playlistItemListResponse', 'etag': 'dTfadgPsawtdlsWoj7taUJ9HzBU', 'nextPageToken': 'EAAaBlBUOkNBSQ', 'items': [{'kind': 'youtube#playlistItem', 'etag': '4PP9KMdQMmkviHBYOzydnPGQkx4', 'id': 'UEx2M1RUQnIxV185dHBwaWtCeEFFX0c2cWpXZEJsakJISi5DNUNCNkIzMEM5Mjg5NkRB', 'snippet': {'publishedAt': '2021-11-14T17:33:25Z', 'channelId': 'UCpzml8N7xz3G_efuqWxY_YQ', 'title': 'Mr. Bean as Palpatine', 'description': 'SUBSCRIBE for more memes! \\n\\nPalpatine got something else on his mind \\n\\nSOCIALS\\nInstagram: https://www.instagram.com/jonkarip/\\nTwitter:  https://twitter.com/JonkariP\\nDiscord: https://discord.gg/WAn8DKT\\n#sequelmeme #meme #shorts\\n\\nBlue Danube (by Strauss) by Strauss\\nCreative Commons â€” Attribution 3.0 Unportedâ€” CC BY 3.0\\nhttps://creativecommons.org/licenses/...\\nMusic provided by FreeMusic109 https://youtube.com/FreeMusic109', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/FItHgP_l234/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/FItHgP_l234/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/FItHgP_l234/hqdefault.jpg', 'width': 480, 'height': 360}, 'standard': {'url': 'https://i.ytimg.com/vi/FItHgP_l234/sddefault.jpg', 'width': 640, 'height': 480}, 'maxres': {'url': 'https://i.ytimg.com/vi/FItHgP_l234/maxresdefault.jpg', 'width': 1280, 'height': 720}}, 'channelTitle': 'EpicDonutDude', 'playlistId': 'PLv3TTBr1W_9tppikBxAE_G6qjWdBljBHJ', 'position': 0, 'resourceId': {'kind': 'youtube#video', 'videoId': 'FItHgP_l234'}, 'videoOwnerChannelTitle': 'Jonkari P', 'videoOwnerChannelId': 'UCUJW2ZNkHOkhD-cWeBFq-gw'}}, {'kind': 'youtube#playlistItem', 'etag': 'jpTCu7mUcjrd3Dra15P9rpqtpvU', 'id': 'UEx2M1RUQnIxV185dHBwaWtCeEFFX0c2cWpXZEJsakJISi45NkJDMEY5NDMwNUQwMjlB', 'snippet': {'publishedAt': '2021-10-26T10:51:22Z', 'channelId': 'UCpzml8N7xz3G_efuqWxY_YQ', 'title': 'Super Weedio Bros. | PhillipLeeX', 'description': \"Heya, itâ€™s me. The guy that runs this stupid-ass channel. \\n\\nI just wanna say **THANK YOU** all for over ðŸ420+ðŸ Subscribers. I started this channel in May 2019, and originally I just uploaded shitty animations I had been doing on my phone. \\nIâ€™ve learned a lot during my time on this channel, but Iâ€™m nowhere near finished. \\n\\nI hope that I can keep doing shit like this, gain even more exposure, experience, and skills.\\nBut for now, I just wanna thank you guys for the kind words and support I've been given.\\n1K Subs, here we come. \\n\\nedit: i never finished this video, and itâ€™s no longer Snoopâ€™s birthday but whatever lol\\n\\nSubscribe to Zzzajac, they helped make this masterpiece. \\n\\nZzzajac:\\nhttps://www.youtube.com/user/EliKrzystof\\n\\n\\nExtended Version:\\nhttps://soundcloud.com/phillip-leex/super-weedio-bros-extended\\n\\nInstrumental Version\\nhttps://soundcloud.com/phillip-leex/phillipleex-super-mario-bros-edm-remix\\n\\n***GIVE CREDIT WHEN USED.***\\n\\n\\n\\n#SnoopDogg #YTPMV #Special\\nFOLLOW ME ON:\\n\\nÂ° Instagram - @phillip.leex \\n\\nÂ° Discord - @PhillipLeeX#4338 \\n\\nÂ° Twitter - @PhillipLeeX \\n\\nÂ° TikTok - @phillipleex\\n\\nðŸš¬ ~plz subscribe. kthxbai~ ðŸš¬\", 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/hVp12W4NU4Q/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/hVp12W4NU4Q/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/hVp12W4NU4Q/hqdefault.jpg', 'width': 480, 'height': 360}, 'standard': {'url': 'https://i.ytimg.com/vi/hVp12W4NU4Q/sddefault.jpg', 'width': 640, 'height': 480}, 'maxres': {'url': 'https://i.ytimg.com/vi/hVp12W4NU4Q/maxresdefault.jpg', 'width': 1280, 'height': 720}}, 'channelTitle': 'EpicDonutDude', 'playlistId': 'PLv3TTBr1W_9tppikBxAE_G6qjWdBljBHJ', 'position': 1, 'resourceId': {'kind': 'youtube#video', 'videoId': 'hVp12W4NU4Q'}, 'videoOwnerChannelTitle': 'PhillipLeeX', 'videoOwnerChannelId': 'UC5QEkVfsyAMm5aNxotPlEOg'}}], 'pageInfo': {'totalResults': 3493, 'resultsPerPage': 2}}\n"
     ]
    }
   ],
   "source": [
    "with build('youtube', 'v3', developerKey=api_key) as youtube:\n",
    "    \n",
    "    request = youtube.playlistItems().list(part='id,snippet', playlistId='PLv3TTBr1W_9tppikBxAE_G6qjWdBljBHJ', maxResults = 2)\n",
    "\n",
    "    response = request.execute()\n",
    "\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Mr. Bean as Palpatine\n",
      "Id: FItHgP_l234\n",
      "Title: Super Weedio Bros. | PhillipLeeX\n",
      "Id: hVp12W4NU4Q\n"
     ]
    }
   ],
   "source": [
    "for result in response['items']:\n",
    "    print('Title: ' + result['snippet']['title'])\n",
    "    print('Id: ' + result['snippet']['resourceId']['videoId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoraping videos from playlist without search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "page: 0\n",
      "page: 1\n",
      "page: 2\n",
      "page: 3\n",
      "page: 4\n",
      "page: 5\n",
      "page: 6\n",
      "page: 7\n",
      "page: 8\n",
      "page: 9\n",
      "page: 10\n",
      "page: 11\n",
      "page: 12\n",
      "page: 13\n",
      "page: 14\n",
      "page: 15\n",
      "page: 16\n",
      "page: 17\n",
      "page: 18\n",
      "page: 19\n",
      "page: 20\n",
      "page: 21\n",
      "page: 22\n",
      "page: 23\n",
      "page: 24\n",
      "page: 25\n",
      "page: 26\n",
      "page: 27\n",
      "page: 28\n",
      "page: 29\n",
      "page: 30\n",
      "page: 31\n",
      "page: 32\n",
      "page: 33\n",
      "page: 34\n",
      "page: 35\n",
      "page: 36\n",
      "page: 37\n",
      "page: 38\n",
      "page: 39\n",
      "page: 40\n",
      "page: 41\n",
      "page: 42\n",
      "page: 43\n",
      "page: 44\n",
      "page: 45\n",
      "page: 46\n",
      "page: 47\n",
      "page: 48\n",
      "page: 49\n",
      "page: 50\n",
      "page: 51\n",
      "page: 52\n",
      "page: 53\n",
      "page: 54\n",
      "page: 55\n",
      "page: 56\n",
      "page: 57\n",
      "page: 58\n",
      "page: 59\n",
      "page: 60\n",
      "page: 61\n",
      "page: 62\n",
      "page: 63\n",
      "page: 64\n",
      "page: 65\n",
      "page: 66\n",
      "page: 67\n",
      "page: 68\n",
      "page: 69\n",
      "page: 70\n",
      "page: 71\n",
      "page: 72\n",
      "page: 73\n",
      "page: 74\n",
      "page: 75\n",
      "page: 76\n",
      "page: 77\n",
      "page: 78\n",
      "page: 79\n",
      "page: 80\n",
      "page: 81\n",
      "page: 82\n",
      "page: 83\n",
      "page: 84\n",
      "page: 85\n",
      "page: 86\n",
      "page: 87\n",
      "page: 88\n",
      "done\n",
      "saving...\n",
      "saved...\n"
     ]
    }
   ],
   "source": [
    "# Scraping youtube video id and title using the YT Api playlistItems function. \n",
    "# Quota cost: 1 per call (each next page request counts as a call)\n",
    "# Downside: needs playlist id to work\n",
    "\n",
    "searchData = []\n",
    "with build('youtube', 'v3', developerKey=api_key) as youtube:\n",
    "    request = youtube.playlistItems().list(part='id,snippet', \n",
    "                                           playlistId='PLLGmt3bXA_93pvHgKm7dbEvW410pDFKKl', \n",
    "                                            fields = 'nextPageToken,items(id,snippet)',\n",
    "                                            maxResults = 50\n",
    "                                            )\n",
    "    \n",
    "    i = 0\n",
    "    print('Start')\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        print('page: ' + str(i))\n",
    "        \n",
    "\n",
    "        for result in response['items']:\n",
    "            \n",
    "            vidId = result['snippet']['resourceId']['videoId']\n",
    "            searchData.append([vidId, result['snippet']['title'], \n",
    "                           'https://i.ytimg.com/vi/' + vidId + '/default.jpg', \n",
    "                           'https://i.ytimg.com/vi/' + vidId + '/mqdefault.jpg',\n",
    "                           'https://i.ytimg.com/vi/' + vidId + '/hqdefault.jpg']\n",
    "                         )\n",
    "\n",
    "        request = youtube.search().list_next(\n",
    "            request, response)\n",
    "        i += 1\n",
    "        \n",
    "    print('done')\n",
    "\n",
    "print('saving...')\n",
    "pd.DataFrame(searchData, columns = ['ID', 'TITLE', 'THUMBNAIL1','THUMBNAIL2','THUMBNAIL3']).to_csv('searchData' + str(round(time.time())) + '.csv', index=False)\n",
    "print('saved...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for playlists and then extracting videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?part=id%2Csnippet&q=learning&maxResults=50&type=playlist&order=videoCount&key=AIzaSyA2uGpj33bOCo1LJO-5UrPkaFaLkA58MkM%0A&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-76549e2767ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                     type = 'playlist', order = 'videoCount')\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# saving all playlist ids in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?part=id%2Csnippet&q=learning&maxResults=50&type=playlist&order=videoCount&key=AIzaSyA2uGpj33bOCo1LJO-5UrPkaFaLkA58MkM%0A&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "# improving the search efficiency by searching for playlists \n",
    "# and then scraping all videos from the resulting playlists\n",
    "# search quota 100 for 50 playlists\n",
    "# in each playlist 1 quota point for 50 videos\n",
    "\n",
    "# already scraped: vlogs, cats, dogs, music, cars, fishing, restoration, tutorial,tech, \n",
    "#                  travel, asmr, lifestyle, Beauty, DIY, learning\n",
    "\n",
    "playlistQuery = 'learning'\n",
    "\n",
    "\n",
    "# searching for playlists\n",
    "with build('youtube', 'v3', developerKey=api_key) as youtube:\n",
    "    \n",
    "    # type = plalist, q = searchword, order = videoCount to get large playlists\n",
    "    request = youtube.search().list(part='id,snippet', q=playlistQuery, maxResults = 50, \n",
    "                                    type = 'playlist', order = 'videoCount')\n",
    "\n",
    "    response = request.execute()\n",
    "\n",
    "# saving all playlist ids in a list\n",
    "\n",
    "playlists = []\n",
    "\n",
    "for result in response['items']:\n",
    "        playlists.append([result['id']['playlistId'],result['snippet']['title']])\n",
    "        \n",
    "\n",
    "# scraping all videos of each playlist\n",
    "\n",
    "for playlist in playlists:\n",
    "    \n",
    "    playlistId = playlist[0]\n",
    "    \n",
    "    print(playlist[1], playlist[0])\n",
    "    \n",
    "    searchData = []\n",
    "    \n",
    "    \n",
    "    with build('youtube', 'v3', developerKey=api_key) as youtube:\n",
    "        \n",
    "        # requesting 50 videos each\n",
    "        request = youtube.playlistItems().list(part='id,snippet', \n",
    "                                               playlistId=playlistId, \n",
    "                                                fields = 'nextPageToken,items(id,snippet)',\n",
    "                                                maxResults = 50\n",
    "                                                )\n",
    "\n",
    "        i = 0\n",
    "        print('Start')\n",
    "        \n",
    "        #requesting new pages as long as there is a return on the request\n",
    "        while request:\n",
    "            try:\n",
    "                response = request.execute()\n",
    "                print('page: ' + str(i))\n",
    "\n",
    "\n",
    "                for result in response['items']:\n",
    "\n",
    "                    # saving the relevant details\n",
    "                    vidId = result['snippet']['resourceId']['videoId']\n",
    "                    searchData.append([vidId, result['snippet']['title'], \n",
    "                                   'https://i.ytimg.com/vi/' + vidId + '/default.jpg', \n",
    "                                   'https://i.ytimg.com/vi/' + vidId + '/mqdefault.jpg',\n",
    "                                   'https://i.ytimg.com/vi/' + vidId + '/hqdefault.jpg']\n",
    "                                 )\n",
    "\n",
    "                request = youtube.search().list_next(\n",
    "                    request, response)\n",
    "                i += 1\n",
    "            except:\n",
    "                print('Some kind of Error.')\n",
    "        print('done')\n",
    "    \n",
    "    # writing to disk after each playlist to not lose everything in case of an error\n",
    "    print('saving...')\n",
    "    pd.DataFrame(searchData, columns = ['ID', 'TITLE', 'THUMBNAIL1','THUMBNAIL2','THUMBNAIL3']).to_csv('ScrapedData/searchData' + str(round(time.time())) + '.csv', index=False, sep=\",\")\n",
    "    print('saved...')\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
