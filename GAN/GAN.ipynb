{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgXVFl5SmZvC"
      },
      "source": [
        "# Simple GAN from the lecures to function as a starting point.\n",
        "Tutorial from https://salu133445.github.io/ismir2019tutorial/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/DataSet(1).zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df9XVlIfhXhY",
        "outputId": "dc098ef6-b626-4930-ddc1-4478dc4032e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mArAbLs2mBhd"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CJ_cVBuhk13r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7367d92a-00d8-4700-e4ff-5e0bbd920ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: livelossplot in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (0.18.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from livelossplot) (5.5.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.2.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2021.11.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.6.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->livelossplot) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip3 install torch torchvision matplotlib tqdm livelossplot termcolor scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zOkT9h38krfZ"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from IPython.display import clear_output\n",
        "from ipywidgets import interact, IntSlider\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from livelossplot import PlotLosses\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "import pandas as pd\n",
        "import os\n",
        "import ast\n",
        "from skimage import io, transform\n",
        "from torchvision import transforms, utils\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VyXuXFtoLxL"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PA14sQ-YoTvW"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "latent_dim = 25*64\n",
        "feature_dim = 120 * 90 * 3\n",
        "n_steps = 10000\n",
        "\n",
        "# Sampler settings\n",
        "sample_interval = 100\n",
        "n_cols = 10\n",
        "n_rows = 2\n",
        "n_samples = 20\n",
        "assert n_samples == n_cols * n_rows, (\n",
        "    \"Number of samples and number of images per column/row do not match.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6fauic_H2wt"
      },
      "source": [
        "## Data loader function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n0OnxrKkeITP"
      },
      "outputs": [],
      "source": [
        "class YThumbnailDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        def from_np_array(array_string):\n",
        "    \n",
        "            return np.array(ast.literal_eval(array_string))\n",
        "        \n",
        "        self.thumbnail_frame = pd.read_csv(csv_file, converters={'VECTOR': from_np_array})\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.thumbnail_frame)\n",
        "    \n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.thumbnail_frame.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        vector = self.thumbnail_frame.iloc[idx, 5].flatten()\n",
        "\n",
        "        vector = np.pad(vector, (0, latent_dim - vector.size), mode='constant', constant_values=0)\n",
        "        sample = {'image': image, 'vector': vector}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_-4efDvfeITR"
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, vector = sample['image'], sample['vector']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "\n",
        "\n",
        "        return {'image': img, 'vector': vector}\n",
        "    \n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, vector = sample['image'], sample['vector']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'vector': torch.from_numpy(vector)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rrO6AL8WeITT"
      },
      "outputs": [],
      "source": [
        "TrainingSet = YThumbnailDataset('/tmp/VectorizedFiles/subFrame12.csv', '/tmp/ImagesLarge/subFrame12', transform=transforms.Compose([Rescale((90,120)),ToTensor(),]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWi87nBLeITU",
        "outputId": "1a74e1b7-f221-4f57-c8e9-62dde69ac90d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 90, 120])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "TrainingSet[34]['image'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh9yTxj5IiHK"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2D-XmLjSr_3p"
      },
      "outputs": [],
      "source": [
        "def images_to_vectors(images):\n",
        "    \"\"\"Convert images to vectors.\"\"\"\n",
        "    return images.view(images.size(0), 90*120*3)\n",
        "\n",
        "def vectors_to_images(vectors):\n",
        "    \"\"\"Convert vectors to images.\"\"\"\n",
        "    return vectors.view(vectors.size(0), 3, 90, 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rrn1nmIQlG"
      },
      "source": [
        "## Neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZWPAxfkmsIWn"
      },
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    \"\"\"A multilayer perceptron (MLP) based generator. The generator takes as\n",
        "    input a latent vector and outputs a fake sample.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden0 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_dim, 256),\n",
        "            torch.nn.LeakyReLU())\n",
        "        self.hidden1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(256, 512),\n",
        "            torch.nn.LeakyReLU())\n",
        "        self.hidden2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512, 1024),\n",
        "            torch.nn.LeakyReLU())\n",
        "        self.out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(1024, feature_dim),\n",
        "            torch.nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden0(x)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorDeconv(torch.nn.Module):\n",
        "    \"\"\"A multilayer perceptron (MLP) based generator. The generator takes as\n",
        "    input a latent vector and outputs a fake sample.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.compression = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 10, (6,3),stride=(6,1), padding=(0,1)),\n",
        "            torch.nn.LeakyReLU())\n",
        "        \n",
        "        self.linear1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2560,2560),\n",
        "            torch.nn.LeakyReLU())\n",
        "        \n",
        "        self.hidden1 = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(10, 6, 3, padding=1),\n",
        "            torch.nn.LeakyReLU())\n",
        "        self.hidden2 = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(6, 6, 4, stride = 2, padding=1),\n",
        "            torch.nn.LeakyReLU())\n",
        "        \n",
        "        self.hidden3 = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(6, 3, (3, 4), stride=(3,4), padding=(3, 4)),\n",
        "            torch.nn.Sigmoid())\n",
        "        \n",
        "        self.out = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(3, 3, 3, stride = 1, padding=1),\n",
        "            torch.nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        " \n",
        "        x = self.compression(torch.reshape(x, (batch_size,1,25,64)))\n",
        "        x = self.linear1(x.reshape(batch_size,2560))\n",
        "        x = self.hidden1(torch.reshape(x, (batch_size,10,16,16)))\n",
        "        x = self.hidden2(x)\n",
        "        x = self.hidden3(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "f7zFBli1n4DN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input = torch.randn(1,1,25,64)\n",
        "\n",
        "print(input.size())\n",
        "downsample = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 10, (6,3),stride=(6,1), padding=(0,1)),\n",
        "            torch.nn.LeakyReLU())\n",
        "\n",
        "layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2560,2560),\n",
        "            torch.nn.LeakyReLU())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "h = downsample(input)\n",
        "\n",
        "print(h.size())\n",
        "\n",
        "h = layer2(h.reshape(1,2560))\n",
        "\n",
        "print(h.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uGLX0B5v6hh",
        "outputId": "c2c1eda5-ca38-48c9-c065-9c721bbb588a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 25, 64])\n",
            "torch.Size([1, 10, 4, 64])\n",
            "torch.Size([1, 2560])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kczm8A8Nl78i"
      },
      "outputs": [],
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    \"\"\"A multilayer perceptron (MLP) based discriminator. The discriminator\n",
        "    takes as input either a real sample (in the training data) or a fake sample\n",
        "    (generated by the generator) and outputs a scalar indicating its authentity.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden0 = torch.nn.Sequential( \n",
        "            torch.nn.Linear(feature_dim, 1024),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout())\n",
        "        self.hidden1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(1024, 512),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout())\n",
        "        self.hidden2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512, 256),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout())\n",
        "        self.out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(256, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden0(x)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminatorConv(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden0 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 3, 4, stride = 2, padding = 2),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout())\n",
        "        \n",
        "        self.hidden1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 3, 4, stride = 2, padding = 2),\n",
        "            torch.nn.LeakyReLU())\n",
        "        \n",
        "        self.hidden2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 3, 4, stride = 2, padding = 2),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout())\n",
        "        \n",
        "        self.hidden3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 3, 4, stride = 2, padding = 2),\n",
        "            torch.nn.LeakyReLU(),\n",
        ")\n",
        "        \n",
        "        self.out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(189,1),\n",
        "            torch.nn.LeakyReLU())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden0(x)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.hidden3(x)\n",
        "        x = self.out(torch.reshape(x,(batch_size,189)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "5tIulQGv9Kyv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiPl8DYCI7pC"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5wngyfaaObas"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
        "    \"\"\"Compute the gradient penalty for regularization. Intuitively, the\n",
        "    gradient penalty help stablize the magnitude of the gradients that the\n",
        "    discriminator provides to the generator, and thus help stablize the training\n",
        "    of the generator.\"\"\"\n",
        "    # Get random interpolations between real and fake samples\n",
        "    alpha = torch.rand(real_samples.size(0), 1).cuda()\n",
        "\n",
        "\n",
        "    interpolates = (alpha * torch.reshape(real_samples, (batch_size, 32400)) + \n",
        "                    ((1 - alpha) * torch.reshape(fake_samples, (batch_size, 32400))\n",
        "))\n",
        "\n",
        "    interpolates = torch.reshape(interpolates, (batch_size, 3,90,120))\n",
        "\n",
        "    interpolates = interpolates.requires_grad_(True)\n",
        "    # Get the discriminator output for the interpolations\n",
        "    d_interpolates = discriminator(interpolates)\n",
        "    # Get gradients w.r.t. the interpolations\n",
        "    fake = torch.ones(real_samples.shape[0], 1).cuda()\n",
        "    gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
        "                                    grad_outputs=fake, create_graph=True,\n",
        "                                    retain_graph=True, only_inputs=True)[0]\n",
        "    # Compute gradient penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x3mgXtVN8ldM"
      },
      "outputs": [],
      "source": [
        "def train_one_step(d_optimizer, g_optimizer, input_vector, real_samples):\n",
        "    \"\"\"Train the networks for one step.\"\"\"\n",
        "    # Sample from the latent distribution\n",
        "    \n",
        "    real_samples = torch.reshape(real_samples.float(), (batch_size,3,90,120))\n",
        "\n",
        "\n",
        "    latent = torch.reshape(input_vector.float(), (batch_size, 25,64))\n",
        "    \n",
        "    # Transfer data to GPU\n",
        "    if torch.cuda.is_available():\n",
        "        real_samples = real_samples.cuda()\n",
        "        \n",
        "        latent = latent.cuda()\n",
        "    \n",
        "    # === Train the discriminator ===\n",
        "    # Reset cached gradients to zero\n",
        "    discriminator.zero_grad()\n",
        "    # Get discriminator outputs for the real samples\n",
        "\n",
        "    prediction_real = discriminator(real_samples)\n",
        "\n",
        "    # Compute the loss function\n",
        "    d_loss_real = torch.mean(torch.nn.functional.relu(1. - prediction_real))\n",
        "    # Backpropagate the gradients\n",
        "    d_loss_real.backward()\n",
        "    \n",
        "    # Generate fake samples with the generator\n",
        "\n",
        "    fake_samples = generator(latent)\n",
        "    # Get discriminator outputs for the fake samples\n",
        "    prediction_fake_d = discriminator(fake_samples.detach())\n",
        "    # Compute the loss function\n",
        "    d_loss_fake = torch.mean(torch.nn.functional.relu(1. + prediction_fake_d))\n",
        "    # Backpropagate the gradients\n",
        "    d_loss_fake.backward()\n",
        "\n",
        "    # Compute gradient penalty\n",
        "    gradient_penalty = 10.0 * compute_gradient_penalty(\n",
        "        discriminator, real_samples.data, fake_samples.data)\n",
        "    # Backpropagate the gradients\n",
        "    gradient_penalty.backward()\n",
        "\n",
        "    # Update the weights\n",
        "    d_optimizer.step()\n",
        "    \n",
        "    # === Train the generator ===\n",
        "    # Reset cached gradients to zero\n",
        "    generator.zero_grad()\n",
        "    # Get discriminator outputs for the fake samples\n",
        "    prediction_fake_g = discriminator(fake_samples)\n",
        "    # Compute the loss function\n",
        "    g_loss = -torch.mean(prediction_fake_g)\n",
        "    # Backpropagate the gradients\n",
        "    g_loss.backward()\n",
        "    # Update the weights\n",
        "    g_optimizer.step()\n",
        "\n",
        "    return d_loss_real + d_loss_fake, g_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ukfh1dxIsDw"
      },
      "source": [
        "## Training preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uCqTBe3p09xY"
      },
      "outputs": [],
      "source": [
        "# Create data loader\n",
        "data_loader = DataLoader(TrainingSet, batch_size=batch_size,\n",
        "                        shuffle=True, num_workers=0, drop_last=True,)\n",
        "\n",
        "latent_loader = DataLoader(TrainingSet, batch_size=batch_size,\n",
        "                        shuffle=True, num_workers=0, drop_last=True,)\n",
        "\n",
        "\n",
        "# Create neural networks\n",
        "discriminator = DiscriminatorConv()\n",
        "generator = GeneratorDeconv()\n",
        "\n",
        "# Create optimizers\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.001, \n",
        "                                betas=(0.0, 0.9))\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001,\n",
        "                                betas=(0.0, 0.9))\n",
        "\n",
        "# Prepare the inputs for the sampler, which will run during the training\n",
        "sample_latent = next(iter(latent_loader))['vector'].float()\n",
        "\n",
        "# Transfer the neural nets and samples to GPU\n",
        "if torch.cuda.is_available():\n",
        "    discriminator = discriminator.cuda()\n",
        "    generator = generator.cuda()\n",
        "    sample_latent = sample_latent.cuda()\n",
        "\n",
        "# Create an empty dictionary to sotre history samples\n",
        "history_samples = {}\n",
        "\n",
        "# Create a LiveLoss logger instance for monitoring\n",
        "groups = {'discriminator': ['d_loss'], 'generator': ['g_loss']}\n",
        "plotlosses = PlotLosses(groups=groups)\n",
        "\n",
        "# Initialize step\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL499fTNJcSd"
      },
      "source": [
        "## Training iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "JsCO34_A3N2U",
        "outputId": "62452925-6af3-4354-d2f1-c6bb601aece7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                                 | 0/10000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-00ef9d92c2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Train the neural networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Record smoothened loss values to LiveLoss logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-53c70d286fd7>\u001b[0m in \u001b[0;36mtrain_one_step\u001b[0;34m(d_optimizer, g_optimizer, input_vector, real_samples)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Compute gradient penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     gradient_penalty = 10.0 * compute_gradient_penalty(\n\u001b[0;32m---> 40\u001b[0;31m         discriminator, real_samples.data, fake_samples.data)\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Backpropagate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-8e6e12c58572>\u001b[0m in \u001b[0;36mcompute_gradient_penalty\u001b[0;34m(discriminator, real_samples, fake_samples)\u001b[0m\n\u001b[1;32m      5\u001b[0m     of the generator.\"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Get random interpolations between real and fake samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Create a progress bar instance for monitoring\n",
        "if step < n_steps + 1:\n",
        "    progress_bar = tqdm(total=n_steps, initial=step, ncols=80, mininterval=1)\n",
        "else:\n",
        "    print(colored('[Warnings]', 'red'), 'Current step exceeds total step.')\n",
        "\n",
        "# Start iterations\n",
        "while step < n_steps + 1:\n",
        "    # Iterate over the dataset\n",
        "    for batch_index, real_batch in enumerate(data_loader):\n",
        "        # Convert input images into vectors\n",
        "\n",
        "        real_samples = images_to_vectors(real_batch['image'])\n",
        "        input_vector = real_batch['vector']\n",
        "\n",
        "\n",
        "        # Train the neural networks\n",
        "        d_loss, g_loss = train_one_step(d_optimizer, g_optimizer, input_vector, real_samples)\n",
        "\n",
        "        # Record smoothened loss values to LiveLoss logger\n",
        "        if step > 0:\n",
        "            running_d_loss = 0.05 * d_loss + 0.95 * running_d_loss\n",
        "            running_g_loss = 0.05 * g_loss + 0.95 * running_g_loss\n",
        "        else:\n",
        "            running_d_loss, running_g_loss = 0.0, 0.0\n",
        "        plotlosses.update({'d_loss': running_d_loss,\n",
        "                         'g_loss': running_g_loss})\n",
        "\n",
        "\n",
        "        # Update losses to progress bar\n",
        "        progress_bar.set_description_str(\n",
        "            \"(d_loss={: 8.6f}, g_loss={: 8.6f})\".format(d_loss, g_loss))\n",
        "        \n",
        "        if step % sample_interval == 0:\n",
        "            # Get generated samples\n",
        "\n",
        "            samples = generator(sample_latent)\n",
        "\n",
        "            samples = torch.split(samples, [n_samples,batch_size-n_samples])[0]\n",
        "\n",
        "            samples = samples.cpu().detach().numpy() \\\n",
        "                             .reshape(n_rows, n_cols, 3, 90, 120) \\\n",
        "                             .transpose((0, 3, 1, 4, 2)) \\\n",
        "                             .reshape(n_rows * 90, n_cols * 120, 3)\n",
        "            history_samples[step] = samples\n",
        "\n",
        "            # Display loss curves\n",
        "            clear_output(True)\n",
        "            if step > 0:\n",
        "                plotlosses.send()\n",
        "            \n",
        "            # Display generated samples\n",
        "            plt.figure(figsize=(15, 3))\n",
        "            plt.imshow(samples, cmap='Greys', vmin=0, vmax=1)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        step += 1\n",
        "        progress_bar.update(1)\n",
        "        if step >= n_steps:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "XqN1sKoODaCm",
        "colab": {
          "referenced_widgets": [
            "a6093ca06c5f415d82476cc0d749cfee",
            "a54e155819f34c0fb75e58eff1bf4a74",
            "d23760483ef649119d3884945474f582",
            "0097bb0d69684a448c5493f2041cc595",
            "e1b30ee23f6c43aba20b72a1f8716b8f",
            "e4b27b91ea6642aa8c75dd2679340784",
            "646da02f164c4711911f65b2067e9492"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "13e4e408-e2f3-479d-f3e7-e8ac2294ddf0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6093ca06c5f415d82476cc0d749cfee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=10000, description='step', layout=Layout(width='500px'), max=10000, step…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x216 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Show history\n",
        "slider = IntSlider(min=0, max=n_steps, value=n_steps, step=sample_interval,\n",
        "                   layout={'width': '500px'})\n",
        "@interact\n",
        "def plot(step=slider):\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    plt.imshow(history_samples[step], cmap='Greys', vmin=0, vmax=1)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUESuerWeITe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a6093ca06c5f415d82476cc0d749cfee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a54e155819f34c0fb75e58eff1bf4a74",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d23760483ef649119d3884945474f582",
              "IPY_MODEL_0097bb0d69684a448c5493f2041cc595"
            ]
          }
        },
        "a54e155819f34c0fb75e58eff1bf4a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d23760483ef649119d3884945474f582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "IntSliderView",
            "style": "IPY_MODEL_e1b30ee23f6c43aba20b72a1f8716b8f",
            "_dom_classes": [],
            "description": "step",
            "step": 100,
            "_model_name": "IntSliderModel",
            "orientation": "horizontal",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "min": 0,
            "continuous_update": true,
            "readout_format": "d",
            "description_tooltip": null,
            "readout": true,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4b27b91ea6642aa8c75dd2679340784"
          }
        },
        "0097bb0d69684a448c5493f2041cc595": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "error",
                "ename": "KeyError",
                "evalue": "ignored",
                "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                  "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipywidgets/widgets/interaction.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_interact_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwarg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0mshow_inline_matplotlib_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_display\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m<ipython-input-226-95f9ce1898af>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(step)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;31mKeyError\u001b[0m: 10000"
                ]
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_646da02f164c4711911f65b2067e9492",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "e1b30ee23f6c43aba20b72a1f8716b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "handle_color": null,
            "_model_name": "SliderStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4b27b91ea6642aa8c75dd2679340784": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "500px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "646da02f164c4711911f65b2067e9492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}